# Chapter 5: LLMè°ƒç”¨ç®¡ç†å™¨

æ¬¢è¿æ¥åˆ° PocketFlow æ•™ç¨‹ä»£ç åº“çŸ¥è¯†ç³»åˆ—çš„ç¬¬äº”ç« ï¼åœ¨ä¸Šä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†[èŠ‚ç‚¹å¤„ç†å•å…ƒ](04_èŠ‚ç‚¹å¤„ç†å•å…ƒ_.md)å¦‚ä½•ä½œä¸ºç³»ç»Ÿçš„"ä¸“ä¸šå·¥ä½œç«™"ï¼Œæ‰§è¡Œå…·ä½“çš„å¤„ç†ä»»åŠ¡ã€‚æœ¬ç« æˆ‘ä»¬å°†æ·±å…¥äº†è§£**LLMè°ƒç”¨ç®¡ç†å™¨**ï¼Œå®ƒå°±åƒæ˜¯æ•´ä¸ªç³»ç»Ÿçš„"AIå¯¹è¯ä¸“å®¶"ï¼Œè´Ÿè´£ä¸å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½äº¤äº’ã€‚

## ä¸ºä»€ä¹ˆéœ€è¦LLMè°ƒç”¨ç®¡ç†å™¨ï¼Ÿ

æƒ³è±¡ä¸€ä¸‹ä½ è¦ä¸ä¸€ä½éå¸¸èªæ˜çš„å¤–å›½ä¸“å®¶åˆä½œï¼Œä½†ä½ ä»¬è¯­è¨€ä¸é€šã€‚ä½ éœ€è¦ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘å®˜æ¥å¸®åŠ©ä½ ï¼š

- ğŸ—£ï¸ **æ²Ÿé€šæ¡¥æ¢**ï¼šåœ¨ç³»ç»Ÿå’ŒAIæ¨¡å‹ä¹‹é—´å»ºç«‹é¡ºç•…çš„é€šä¿¡
- ğŸ”„ **é”™è¯¯å¤„ç†**ï¼šå½“ç½‘ç»œä¸ç¨³å®šæˆ–AIå“åº”å¼‚å¸¸æ—¶è‡ªåŠ¨é‡è¯•
- ğŸ’¾ **æ™ºèƒ½ç¼“å­˜**ï¼šé¿å…é‡å¤è¯¢é—®ç›¸åŒçš„é—®é¢˜ï¼ŒèŠ‚çœæ—¶é—´å’Œèµ„æº
- ğŸ“Š **æ—¥å¿—è®°å½•**ï¼šè¯¦ç»†è®°å½•æ¯æ¬¡å¯¹è¯ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–

LLMè°ƒç”¨ç®¡ç†å™¨å°±æ˜¯è¿™æ ·çš„"AIç¿»è¯‘å®˜"ï¼Œå®ƒç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆã€ç¨³å®šåœ°ä¸å„ç§å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚

## LLMè°ƒç”¨ç®¡ç†å™¨çš„å·¥ä½œåŸç†

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥ç†è§£LLMè°ƒç”¨ç®¡ç†å™¨æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ä¸»è¦çš„ä»£ç ä½äº `utils/call_llm.py` æ–‡ä»¶ä¸­ï¼š

```python
def call_llm(prompt: str, use_cache: bool = True) -> str:
    # è®°å½•æç¤ºè¯
    logger.info(f"PROMPT: {prompt}")
    
    # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œå…ˆæ£€æŸ¥ç¼“å­˜
    if use_cache:
        cache = load_cache()
        if prompt in cache:
            logger.info(f"RESPONSE: {cache[prompt]}")
            return cache[prompt]  # ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ
    
    # æ ¹æ®é…ç½®é€‰æ‹©LLMæä¾›å•†
    provider = get_llm_provider()
    if provider == "GEMINI":
        response_text = _call_llm_gemini(prompt)
    else:
        response_text = _call_llm_provider(prompt)
    
    # è®°å½•å“åº”
    logger.info(f"RESPONSE: {response_text}")
    
    # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œä¿å­˜ç»“æœ
    if use_cache:
        cache = load_cache()
        cache[prompt] = response_text
        save_cache(cache)
    
    return response_text
```

è¿™æ®µä»£ç å±•ç¤ºäº†LLMè°ƒç”¨ç®¡ç†å™¨çš„æ ¸å¿ƒå·¥ä½œæµç¨‹ï¼šæ£€æŸ¥ç¼“å­˜ â†’ é€‰æ‹©æä¾›å•† â†’ è°ƒç”¨API â†’ è®°å½•æ—¥å¿— â†’ ä¿å­˜ç¼“å­˜ã€‚

## æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 1. å¤šæä¾›å•†æ”¯æŒ

LLMè°ƒç”¨ç®¡ç†å™¨æ”¯æŒå¤šç§AIæ¨¡å‹æä¾›å•†ï¼Œå°±åƒå¤šè¯­ç§ç¿»è¯‘å®˜ï¼š

```python
def get_llm_provider():
    provider = os.getenv("LLM_PROVIDER")
    if not provider and (os.getenv("GEMINI_PROJECT_ID") or os.getenv("GEMINI_API_KEY")):
        provider = "GEMINI"  # é»˜è®¤ä½¿ç”¨Google Gemini
    return provider
```

ç³»ç»Ÿé€šè¿‡ç¯å¢ƒå˜é‡æ¥é…ç½®ä½¿ç”¨å“ªä¸ªAIæœåŠ¡ï¼Œæ”¯æŒGoogle Geminiã€OpenAIå…¼å®¹çš„APIç­‰ã€‚

### 2. æ™ºèƒ½ç¼“å­˜æœºåˆ¶

ä¸ºäº†é¿å…é‡å¤è°ƒç”¨ç›¸åŒçš„æç¤ºè¯ï¼ŒLLMè°ƒç”¨ç®¡ç†å™¨å®ç°äº†æ™ºèƒ½ç¼“å­˜ï¼š

```python
def load_cache():
    try:
        with open(cache_file, 'r') as f:
            return json.load(f)  # ä»æ–‡ä»¶åŠ è½½ç¼“å­˜
    except:
        logger.warning("æ— æ³•åŠ è½½ç¼“å­˜")
    return {}

def save_cache(cache):
    try:
        with open(cache_file, 'w') as f:
            json.dump(cache, f)  # ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶
    except:
        logger.warning("æ— æ³•ä¿å­˜ç¼“å­˜")
```

è¿™å°±åƒèªæ˜çš„ç¿»è¯‘å®˜ä¼šè®°å½•ä¸‹å·²ç»ç¿»è¯‘è¿‡çš„å†…å®¹ï¼Œä¸‹æ¬¡é‡åˆ°ç›¸åŒçš„é—®é¢˜ç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚

### 3. å®Œå–„çš„é”™è¯¯å¤„ç†

LLMè°ƒç”¨ç®¡ç†å™¨å†…ç½®äº†å¤šç§é”™è¯¯å¤„ç†æœºåˆ¶ï¼š

```python
try:
    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 
    return response.json()["choices"][0]["message"]["content"]
except requests.exceptions.HTTPError as e:
    error_message = f"HTTPé”™è¯¯: {e}"
    # è¯¦ç»†é”™è¯¯ä¿¡æ¯å¤„ç†...
except requests.exceptions.ConnectionError:
    raise Exception("æ— æ³•è¿æ¥åˆ°APIï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
except requests.exceptions.Timeout:
    raise Exception("è¯·æ±‚è¶…æ—¶")
```

è¿™ç§è®¾è®¡ç¡®ä¿äº†å³ä½¿åœ¨ç½‘ç»œä¸ç¨³å®šæˆ–æœåŠ¡å¼‚å¸¸çš„æƒ…å†µä¸‹ï¼Œç³»ç»Ÿä¹Ÿèƒ½ä¼˜é›…åœ°å¤„ç†é”™è¯¯ã€‚

## å®é™…å·¥ä½œæµç¨‹

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªåºåˆ—å›¾æ¥çœ‹çœ‹LLMè°ƒç”¨ç®¡ç†å™¨åœ¨å®Œæ•´æµç¨‹ä¸­çš„è§’è‰²ï¼š

```mermaid
sequenceDiagram
    participant å¤„ç†èŠ‚ç‚¹
    participant LLMè°ƒç”¨ç®¡ç†å™¨
    participant ç¼“å­˜ç³»ç»Ÿ
    participant AIæœåŠ¡æä¾›å•†
    
    å¤„ç†èŠ‚ç‚¹->>LLMè°ƒç”¨ç®¡ç†å™¨: å‘é€æç¤ºè¯å’Œé…ç½®
    LLMè°ƒç”¨ç®¡ç†å™¨->>ç¼“å­˜ç³»ç»Ÿ: æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜ç»“æœ
    ç¼“å­˜ç³»ç»Ÿ-->>LLMè°ƒç”¨ç®¡ç†å™¨: è¿”å›ç¼“å­˜ç»“æœï¼ˆå¦‚æœ‰ï¼‰
    
    alt æœ‰ç¼“å­˜ç»“æœ
        LLMè°ƒç”¨ç®¡ç†å™¨-->>å¤„ç†èŠ‚ç‚¹: ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ
    else æ— ç¼“å­˜ç»“æœ
        LLMè°ƒç”¨ç®¡ç†å™¨->>AIæœåŠ¡æä¾›å•†: å‘é€APIè¯·æ±‚
        AIæœåŠ¡æä¾›å•†-->>LLMè°ƒç”¨ç®¡ç†å™¨: è¿”å›AIå“åº”
        LLMè°ƒç”¨ç®¡ç†å™¨->>ç¼“å­˜ç³»ç»Ÿ: ä¿å­˜ç»“æœåˆ°ç¼“å­˜
        LLMè°ƒç”¨ç®¡ç†å™¨-->>å¤„ç†èŠ‚ç‚¹: è¿”å›AIå“åº”
    end
```

## LLMè°ƒç”¨ç®¡ç†å™¨çš„å†…éƒ¨å®ç°

### Google Gemini é›†æˆ

å¯¹äºGoogle GeminiæœåŠ¡ï¼ŒLLMè°ƒç”¨ç®¡ç†å™¨ä½¿ç”¨ä¸“é—¨çš„å‡½æ•°ï¼š

```python
def _call_llm_gemini(prompt: str) -> str:
    # é…ç½®Geminiå®¢æˆ·ç«¯
    if os.getenv("GEMINI_PROJECT_ID"):
        client = genai.Client(vertexai=True, project=os.getenv("GEMINI_PROJECT_ID"))
    else:
        client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
    
    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹
    model = os.getenv("GEMINI_MODEL", "gemini-2.5-pro-exp-03-25")
    response = client.models.generate_content(model=model, contents=[prompt])
    return response.text
```

### é€šç”¨APIæä¾›å•†æ”¯æŒ

å¯¹äºå…¶ä»–OpenAIå…¼å®¹çš„APIæä¾›å•†ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•ï¼š

```python
def _call_llm_provider(prompt: str) -> str:
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    provider = os.environ.get("LLM_PROVIDER")
    model = os.environ.get(f"{provider}_MODEL")
    base_url = os.environ.get(f"{provider}_BASE_URL")
    api_key = os.environ.get(f"{provider}_API_KEY", "")
    
    # æ„å»ºè¯·æ±‚URLå’Œå‚æ•°
    url = f"{base_url.rstrip('/')}/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"
    
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,  # æ§åˆ¶åˆ›é€ æ€§ç¨‹åº¦
    }
    
    # å‘é€è¯·æ±‚å¹¶è¿”å›ç»“æœ
    response = requests.post(url, headers=headers, json=payload)
    return response.json()["choices"][0]["message"]["content"]
```

## æ—¥å¿—è®°å½•ç³»ç»Ÿ

LLMè°ƒç”¨ç®¡ç†å™¨è¿˜åŒ…å«å®Œå–„çš„æ—¥å¿—è®°å½•åŠŸèƒ½ï¼š

```python
# é…ç½®æ—¥å¿—ç³»ç»Ÿ
log_directory = os.getenv("LOG_DIR", "logs")
os.makedirs(log_directory, exist_ok=True)
log_file = os.path.join(log_directory, f"llm_calls_{datetime.now().strftime('%Y%m%d')}.log")

logger = logging.getLogger("llm_logger")
logger.setLevel(logging.INFO)
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
logger.addHandler(file_handler)
```

æ¯æ¬¡è°ƒç”¨éƒ½ä¼šè¯¦ç»†è®°å½•æç¤ºè¯å’Œå“åº”ï¼Œä¾¿äºåç»­åˆ†æå’Œä¼˜åŒ–ã€‚

## å®é™…ä½¿ç”¨ç¤ºä¾‹

åœ¨èŠ‚ç‚¹å¤„ç†å•å…ƒä¸­ï¼ŒLLMè°ƒç”¨ç®¡ç†å™¨è¢«è¿™æ ·ä½¿ç”¨ï¼š

```python
class IdentifyAbstractions(Node):
    def exec(self, prep_res):
        # å‡†å¤‡æç¤ºè¯
        prompt = f"""
åˆ†æä»£ç åº“ä¸Šä¸‹æ–‡...
è¯†åˆ«æ ¸å¿ƒæŠ½è±¡æ¦‚å¿µ...
"""
        
        # è°ƒç”¨LLMï¼ˆå¯ç”¨ç¼“å­˜ï¼Œé™¤éæ­£åœ¨é‡è¯•ï¼‰
        response = call_llm(prompt, use_cache=(use_cache and self.cur_retry == 0))
        
        # å¤„ç†å“åº”
        yaml_str = response.strip().split("```yaml")[1].split("```")[0].strip()
        abstractions = yaml.safe_load(yaml_str)
        
        return abstractions
```

## é…ç½®ç¯å¢ƒå˜é‡

è¦ä½¿ç”¨LLMè°ƒç”¨ç®¡ç†å™¨ï¼Œéœ€è¦é…ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡ï¼š

```bash
# ä½¿ç”¨Google Gemini
export LLM_PROVIDER=GEMINI
export GEMINI_API_KEY=your_api_key_here
export GEMINI_MODEL=gemini-2.5-pro-exp-03-25

# æˆ–è€…ä½¿ç”¨OpenAIå…¼å®¹çš„API
export LLM_PROVIDER=OLLAMA
export OLLAMA_MODEL=llama3.1
export OLLAMA_BASE_URL=http://localhost:11434
```

## é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

LLMè°ƒç”¨ç®¡ç†å™¨ä¸èŠ‚ç‚¹å¤„ç†å•å…ƒçš„é‡è¯•æœºåˆ¶å®Œç¾é…åˆï¼š

```python
# åœ¨èŠ‚ç‚¹ä¸­ï¼ŒLLMè°ƒç”¨å¤±è´¥æ—¶ä¼šè§¦å‘é‡è¯•
class IdentifyAbstractions(Node):
    def __init__(self, max_retries=5, wait=20):
        super().__init__(max_retries, wait)  # ç»§æ‰¿é‡è¯•é…ç½®
    
    def exec(self, prep_res):
        try:
            response = call_llm(prompt, use_cache=True)
            # å¤„ç†å“åº”...
        except Exception as e:
            print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            if self.cur_retry < self.max_retries:
                print(f"ç¬¬{self.cur_retry+1}æ¬¡é‡è¯•...")
                time.sleep(self.wait)
                self.cur_retry += 1
                return self.exec(prep_res)  # é‡è¯•
            else:
                raise e  # é‡è¯•æ¬¡æ•°ç”¨å°½
```

## æ€»ç»“

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬äº†è§£äº†LLMè°ƒç”¨ç®¡ç†å™¨çš„æ ¸å¿ƒä½œç”¨ï¼š

- ğŸŒ **å¤šè¯­è¨€ä¸“å®¶**ï¼šæ”¯æŒå¤šç§AIæœåŠ¡æä¾›å•†ï¼Œçµæ´»é€‚é…ä¸åŒéœ€æ±‚
- ğŸ’¾ **æ™ºèƒ½è®°å¿†å®˜**ï¼šé€šè¿‡ç¼“å­˜æœºåˆ¶é¿å…é‡å¤è°ƒç”¨ï¼Œæé«˜æ•ˆç‡
- ğŸ›¡ï¸ **å¯é é€šä¿¡å‘˜**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ç¡®ä¿ç¨³å®šæ€§
- ğŸ“Š **è¯¦ç»†è®°å½•è€…**ï¼šå®Œæ•´çš„æ—¥å¿—ç³»ç»Ÿä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–
- ğŸ”§ **é…ç½®ç®¡ç†å¸ˆ**ï¼šé€šè¿‡ç¯å¢ƒå˜é‡è½»æ¾é…ç½®ä¸åŒAIæœåŠ¡

LLMè°ƒç”¨ç®¡ç†å™¨å°±åƒæ˜¯æ•™ç¨‹ç”Ÿæˆç³»ç»Ÿçš„"AIå¯¹è¯ä¸“å®¶"ï¼Œå®ƒç¡®ä¿æ¯ä¸ªå¤„ç†èŠ‚ç‚¹éƒ½èƒ½é«˜æ•ˆã€å¯é åœ°ä¸å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œä¸ºç”Ÿæˆé«˜è´¨é‡çš„æ•™ç¨‹å†…å®¹æä¾›å¼ºå¤§çš„AIèƒ½åŠ›æ”¯æŒã€‚

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢[æŠ½è±¡æ¦‚å¿µè¯†åˆ«å™¨](06_æŠ½è±¡æ¦‚å¿µè¯†åˆ«å™¨_.md)ï¼Œå­¦ä¹ ç³»ç»Ÿå¦‚ä½•åˆ†æä»£ç åº“å¹¶è¯†åˆ«å‡ºæ ¸å¿ƒçš„æŠ½è±¡æ¦‚å¿µã€‚è®©æˆ‘ä»¬ç»§ç»­è¿™ä¸ªç²¾å½©çš„å­¦ä¹ ä¹‹æ—…ï¼

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)